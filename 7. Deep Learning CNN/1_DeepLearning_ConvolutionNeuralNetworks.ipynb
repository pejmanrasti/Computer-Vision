{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_DeepLearning_ConvolutionNeuralNetworks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM4OTn7K+IdHGfUcwCwSqRO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worklifesg/Python-for-Computer-Vision-with-OpenCV-and-Deep-Learning/blob/main/7.%20Deep%20Learning%20CNN/1_DeepLearning_ConvolutionNeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWDl_nLoM_Q"
      },
      "source": [
        "### Deep Learning For Object Detection and Image Classification\r\n",
        "\r\n",
        "Here in this section we will cover only applications of Deep Learning for Image Classification problems.\r\n",
        "\r\n",
        " - <b> Convolution Neural Networks (CNN) Theory </b>\r\n",
        " - Keras CNN with MNIST Dataset\r\n",
        " - Keras CNN with CIFAR-10\r\n",
        " - Deep Learning on Custom Images (Cats & Dogs)\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFXAfAwUo4DC"
      },
      "source": [
        "#### Convolution Neural Networks (CNN) Theory \r\n",
        "\r\n",
        "Basically when dealing in 2D data (images) we need to tackle different color channels and use <b> tensors </b> to form our data correctly.\r\n",
        "\r\n",
        "Convolution Neural Networks (CNN) is a simple perceptron model <b> (Biological research - Neurons can focus on small local receptive field when activated). </b>\r\n",
        " - Yann LeCun et al. published their work in 1998 linking Artificial Neural Network (ANN) with images that resulted in CNN and implemented in LeNet-5 architecture to classify MNIST dataset.\r\n",
        " - A simple CNN architecture looks like this:\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=1q33DA7IetuaQ7Qmb9b05ur-uSz7d3XoA)\r\n",
        "\r\n",
        " - As we have see in CNN, we have <b> Input, </b> <b> Feature Extraction (convolution, sub-sampling, pooling)</b> and <b> Classification </b>. So we can break out in following sections to learn deep into CNN.\r\n",
        "  - Tensors, DNN vs CNN, Convolution & Filters\r\n",
        "  - Padding, Pooling Layers, & Review Dropout\r\n",
        "\r\n",
        "##### Tensors\r\n",
        "\r\n",
        "- N-Dimensional Arrays - Scaler (1), Vector (2), Matrix (3), Tensor (4)\r\n",
        "- Tensors are very convenient to feed set of images <b> (I,H,W,C)</b>, where:\r\n",
        " - I = Images\r\n",
        " - H = Height of images in pixels\r\n",
        " - W = Width of images in pixels\r\n",
        " - C = Color Channels: 1-Gray Scale, 3-RGB\r\n",
        "\r\n",
        "##### DNN vs CNN\r\n",
        "\r\n",
        "- Densely Connected Layer:\r\n",
        " - Every neuron in one layer is connected to every other neuron in the another layer.\r\n",
        "- Convolution Layer:\r\n",
        " - Every neuron in one layer is connected to <b> nearby neurons only </b> directly inspired by <b> biological visual cortex where you are only looking at local receptive fields</b>\r\n",
        "\r\n",
        "![alt text](https://drive.google.com/uc?id=1F2pr8Ov1LbJWLfNKZvjsrS5ok66-qFac)\r\n",
        "\r\n",
        "- <b> Why consider CNN over DNN ?\r\n",
        " - In general practice we have $28\\times28$ pixels data but in reality we have images that are atleast $256\\times256$ pixels data i.e. $\\approx 56000$ total pixels that leads to too many parameters, unscalable to new images. \r\n",
        " - CNN have a major advantage: \r\n",
        "   - For IMAGE PROCESSING: </b> as nearby pixels are much coorealted to each other and helpful in image detection \r\n",
        "   - Each CNN layer looks at an increasing part of the image.\r\n",
        "   - Helps in INVARIANCE: by having units only connected to nearby units\r\n",
        "   - CNN also helps with regularization, limiting search of weights to the size of the convolution.\r\n",
        "\r\n",
        "##### CNN \r\n",
        "\r\n",
        "- Generally we have input layer and for each pixels there are nearby neurons connected to the convolution layer.\r\n",
        "- <b> At the edge of the image there are issues as there may not be enough input neurons from the actual data. This is fixed by adding padding of zeros around it. </b>\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=1fHLYmzAXM4hpuikxyYvduBYeZOYNkTuo)\r\n",
        "\r\n",
        "- 1D Convolution\r\n",
        " - We can treat these weights as a filter\r\n",
        " - Example of 1 filter\r\n",
        "\r\n",
        " \\begin{align*}\r\n",
        " y = w_1x_1+w_2x_2\r\n",
        " \\end{align*}\r\n",
        "\r\n",
        " - If $(w_1,w_2)=(1,-1)$ then $y=x_1-x_2$ and $y_{\\mathrm{max}}$ (activation) occurs at $(x_1,x_2)=(1,0)$ - <b> This is an exmaple of edge detection.</b>\r\n",
        " \r\n",
        " ![alt text](https://drive.google.com/uc?id=1kkdQcfeLd92YaoFoojTejZIYqNxxIfGc)\r\n",
        "\r\n",
        "\r\n",
        "- When we have multiple weights across input neurons repeated consecutive times - <b> stride of filter </b>. In this case we have 1 filter, 2 filter size (weigths) and stride = 2.\r\n",
        "- Also, we can have single stride as well but last neuron in this case might be left out that can be added with padding.\r\n",
        "- Plus we can increase number of filters of 2 or 4 as shown in the figure. Each filter will be detecting different features\r\n",
        "- For simplicity, we present the CNN network as set of blocks\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=1LlTaY11T1MHkSEmAfpdXEiurhNG9sujH)\r\n",
        "\r\n",
        "- 2D Convolution\r\n",
        " - Now we have height and width of image itself ans subsections are directly related to tensors\r\n",
        " - Similarly we can have color added to input image if it is colored image.\r\n",
        "\r\n",
        "![alt text](https://drive.google.com/uc?id=1SZ0haGvqk1tCvTn6eEAI2kLYvsCOxvDQ)\r\n",
        "\r\n",
        "- CNN Filters\r\n",
        " - Looked as grid system.\r\n",
        " - Input image (padded already) has [-1,0,1] where -1 is lightest and +1 is the darkest.\r\n",
        " - $3\\times3$ filter is multiplied with input image $(6\\times6)$ resulting in $3\\times3$ matrix for first pixel whose sum gives us total pixel value.\r\n",
        "\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=1G9Grq8rlLtPrvFr6SQcSyRPg-hE3w3Yr)\r\n",
        "\r\n",
        " - Taking into account stride distance (how fast you are moving across image with filters)\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=12lijzKVkXBYratAjoxvSrPRwiAIK5O0P)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McO8NhCqFuFi"
      },
      "source": [
        "##### Sub-Sampling (Pooling)\r\n",
        "\r\n",
        "<b> Pooling layers will subsample the input image, which reduces the memory use and computer load as well as reducing the number of parameters.</b>\r\n",
        "\r\n",
        "- For an image, we take a $n\\times n$ pool (generally we take $2\\times2$ pooling), from which we evaluate maximum value.\r\n",
        "- We decide that mximum value is going to be the <b> representative </b> of the entire kernel.\r\n",
        "\r\n",
        " ![alt text](https://drive.google.com/uc?id=11OrR7JfXRxEkF7DIFKSkWJBwlZ3exd_a)\r\n",
        "\r\n",
        " - <b> Pooling layer </b> ends up removing a lot of information (even $2\\times2$) can remove 75% of the input data. But for CNN for iamges it is just too computational expensive.\r\n",
        "\r\n",
        "##### Dropout\r\n",
        "\r\n",
        "<b> Form of regularization to help prevent overfitting. </b>\r\n",
        " - During training, units are randomly dropped along with their connections\r\n",
        "\r\n",
        "##### Famours CNN architectures\r\n",
        "\r\n",
        " - LeNet-5 by Yann LeCun\r\n",
        " - AlexNet \r\n",
        " - GoogLeNet\r\n",
        " - ResNet\r\n",
        "\r\n",
        "AlexNet\r\n",
        "\r\n",
        "![alt text](https://drive.google.com/uc?id=1kEP-nECVSCCCmMP_7sm4elB-f_SseGuq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRmdKU2CJPAV"
      },
      "source": [
        "<h4 align='center'> Keras CNN with MNIST </h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJIXDhuvlD_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81a0a0a-23b9-46e9-f52b-3ed4195714d4"
      },
      "source": [
        "# Checking GPU available on Colab\r\n",
        "\r\n",
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)\r\n",
        "\r\n",
        "from psutil import virtual_memory\r\n",
        "ram_gb = virtual_memory().total / 1e9\r\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\r\n",
        "\r\n",
        "if ram_gb < 20:\r\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\r\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\r\n",
        "  print('re-execute this cell.')\r\n",
        "else:\r\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Dec 13 03:57:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viktqHvgIr03"
      },
      "source": [
        "#import libraries\r\n",
        "\r\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24I_u1htJbvd"
      },
      "source": [
        "(x_train,y_train) , (x_test,y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phi2RcTvJk-0",
        "outputId": "eaa9543d-08af-4721-881e-be0a42cca722"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "print('Training Dataset Size: ', x_train.shape)\r\n",
        "print('Testing Dataset Size: ', x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Size:  (60000, 28, 28)\n",
            "Testing Dataset Size:  (10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRLTLDTJz0c"
      },
      "source": [
        "Here we see we have 60000 training images with $28\\times28$ pixel size with no color channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "j4K6ViCLJy41",
        "outputId": "f219955d-362a-41e1-ce31-84e7ddf6315e"
      },
      "source": [
        "single_image = x_train[0]\r\n",
        "#plt.imshow(single_image,cmap='gray') for black background\r\n",
        "plt.imshow(single_image,cmap='gray_r')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb7655929e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MbCxBrNKFuM",
        "outputId": "47bf93d1-7c6a-4ab7-bd5c-96bb727891bd"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1XWyIxpKWJ8"
      },
      "source": [
        "It seems to have different output values and network can get confused and think it as a regression problem instead of image classification problem.\r\n",
        "\r\n",
        " - So we need to standradize them using one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS2nQHRJKUns",
        "outputId": "d5637d4e-23e2-436c-a8cb-53f0f8313f4a"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\r\n",
        "\r\n",
        "y_cat_test = to_categorical(y_test,10)\r\n",
        "y_cat_train = to_categorical(y_train,10)\r\n",
        "\r\n",
        "print('First Index')\r\n",
        "print(y_train[0])\r\n",
        "print(y_cat_train[0])\r\n",
        "print('------------------------------------------------')\r\n",
        "print('Second Index')\r\n",
        "print(y_train[1])\r\n",
        "print(y_cat_train[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Index\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "------------------------------------------------\n",
            "Second Index\n",
            "0\n",
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ6i4NhxLaGS"
      },
      "source": [
        "Also the image is not normalize as it has values ranging between [0-255]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlf7zDA0LEr7",
        "outputId": "892b6e36-7dea-466d-afc8-f1ae95a88e21"
      },
      "source": [
        "single_image.max()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXAlFeb_LisS",
        "outputId": "fe6f7abf-9940-4ad1-b67d-469098fc95d1"
      },
      "source": [
        "#can use sklearn directly to normalize but let us see the method to normalize if we don't ahver sklearn\r\n",
        "\r\n",
        "x_train = x_train/x_train.max()\r\n",
        "x_test = x_test/x_test.max()\r\n",
        "\r\n",
        "single_image = x_train[0]\r\n",
        "single_image.max()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLF4hcAEL55q"
      },
      "source": [
        "#scaling the data (generalized network to work on any image data)\r\n",
        "\r\n",
        "x_train = x_train.reshape(60000,28,28,1) #just telling there is a channel (color)\r\n",
        "x_test= x_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGCQJLFSMbIJ"
      },
      "source": [
        "#Build and train out model now\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,Conv2D,MaxPool2D,Flatten"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa2-x8XDMpXp"
      },
      "source": [
        "model = Sequential() #create a model\r\n",
        "\r\n",
        "#Conv Layer\r\n",
        "model.add(Conv2D(filters=32,kernel_size=(4,4),\r\n",
        "                 input_shape=(28,28,1),activation='relu'))\r\n",
        "#Pooling layer\r\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\r\n",
        "\r\n",
        "#Flatten out to understand Dense layer (2D-->1D)\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "#Dense Layer\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "\r\n",
        "#Output Layer\r\n",
        "model.add(Dense(10,activation='softmax'))\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='rmsprop',\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rv0GFxaNtHA",
        "outputId": "6930b2fb-adfc-45ea-82cc-bff8b8ea21c4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 25, 25, 32)        544       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               589952    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 591,786\n",
            "Trainable params: 591,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lroTi_irNvs4",
        "outputId": "bd64cf5b-c915-44c4-8404-e77cf31e4c11"
      },
      "source": [
        "#Train model\r\n",
        "\r\n",
        "model.fit(x_train,y_cat_train,epochs=50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1431 - accuracy: 0.9564\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0486 - accuracy: 0.9848\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0354 - accuracy: 0.9897\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0274 - accuracy: 0.9923\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0225 - accuracy: 0.9937\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0192 - accuracy: 0.9951\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0156 - accuracy: 0.9957\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0129 - accuracy: 0.9963\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0103 - accuracy: 0.9974\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0090 - accuracy: 0.9974\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0070 - accuracy: 0.9981\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0067 - accuracy: 0.9984\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0056 - accuracy: 0.9984\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0051 - accuracy: 0.9986\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0040 - accuracy: 0.9989\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0033 - accuracy: 0.9991\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0028 - accuracy: 0.9994\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.9994\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0015 - accuracy: 0.9995\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0018 - accuracy: 0.9995\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0014 - accuracy: 0.9995\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 7.3954e-04 - accuracy: 0.9997\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0010 - accuracy: 0.9997\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 5.0499e-04 - accuracy: 0.9999\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 4.0783e-04 - accuracy: 0.9999\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 5.0375e-04 - accuracy: 0.9999\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.1033e-04 - accuracy: 0.9999\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2799e-04 - accuracy: 0.9999\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0447e-04 - accuracy: 0.9999\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 3.7025e-04 - accuracy: 0.9999\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3881e-04 - accuracy: 0.9999\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1371e-04 - accuracy: 0.9999\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5942e-05 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 3.4979e-05 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6463e-04 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 3.4141e-05 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 3.4061e-05 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 6.0182e-06 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 3.3435e-05 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3480e-06 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 4.6047e-06 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 9.4366e-09 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 4.9419e-07 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 4.0631e-08 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.0537e-08 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7147e-04 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8091e-05 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 4.9173e-08 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb7504aceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVl8l1JtQCp5"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0SxAUHPN7jX",
        "outputId": "fa969ccf-5d0d-4bcb-d723-2cb6a0ca07f6"
      },
      "source": [
        "print('Metrics used: ', model.metrics_names)\r\n",
        "\r\n",
        "#evaluate using test dataset\r\n",
        "\r\n",
        "model.evaluate(x_test,y_cat_test)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics used:  ['loss', 'accuracy']\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2285 - accuracy: 0.9872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22847983241081238, 0.9872000217437744]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgsoXjKVQWqu",
        "outputId": "57fecfb0-3605-4421-d2fd-66fb03d2fc86"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "predictions = model.predict_classes(x_test)\r\n",
        "\r\n",
        "print('First Index')\r\n",
        "print('Testing Dataset: ',y_cat_test[0])\r\n",
        "print('Predicted value: ',predictions[0])\r\n",
        "print('------------------------------------------------')\r\n",
        "print('Second Index')\r\n",
        "print('Testing Dataset: ',y_cat_test[1])\r\n",
        "print('Predicted value: ',predictions[1])\r\n",
        "\r\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-5769844500fe>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "First Index\n",
            "Testing Dataset:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Predicted value:  7\n",
            "------------------------------------------------\n",
            "Second Index\n",
            "Testing Dataset:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Predicted value:  2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.99      0.98      0.98      1032\n",
            "           3       0.99      0.99      0.99      1010\n",
            "           4       0.99      0.99      0.99       982\n",
            "           5       0.99      0.99      0.99       892\n",
            "           6       0.99      0.99      0.99       958\n",
            "           7       0.99      0.98      0.99      1028\n",
            "           8       0.98      0.99      0.98       974\n",
            "           9       0.99      0.98      0.98      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}